{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9761a26",
   "metadata": {},
   "source": [
    "# Linear Algebra Review\n",
    "\n",
    "**Basic Notation**\n",
    "\n",
    "Linear algebra provides a way of compactly representing and operating on sets of linear equations. For example:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "4x_{1}-5x_{2} &= -13 \\\\\n",
    "-2x_{1}+3x_{2} &= 9\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "<p style=\"text-align:center\">The matrix notation of above equations is:</p>\n",
    "\n",
    "$$Ax =b$$\n",
    "\n",
    "$$\\text{with } A = \\begin{bmatrix}\n",
    "      4 & -5 \\\\\n",
    "      -2 & 3\n",
    "      \\end{bmatrix}, \\enspace b = \\begin{bmatrix}\n",
    "      -13 \\\\\n",
    "      9\n",
    "      \\end{bmatrix}$$\n",
    "\n",
    "By $A \\in \\mathbb{R}^{m \\times n}$, we denote a matrix with $m$ rows and $n$ columns. By $x \\in \\mathbb{R}^n$, we denote vector with $n$ entries.\n",
    "\n",
    "$$\\begin{align}A = \\begin{bmatrix}\n",
    "      a_{11} & a_{12} & a_{13} & ... & a_{1n} \\\\\n",
    "      a_{21} & a_{22} & a_{23} & ... & a_{2n} \\\\\n",
    "      a_{31} & a_{32} & a_{33} & ... & a_{3n} \\\\\n",
    "      ... & ... & ... & ... & ... \\\\\n",
    "      a_{m1} & a_{m2} & a_{m3} & ... & a_{mn}\n",
    "      \\end{bmatrix}, \\enspace \\enspace x = \\begin{bmatrix}\n",
    "      x_{1} \\\\\n",
    "      x_{2} \\\\\n",
    "      x_{3} \\\\\n",
    "      .. \\\\\n",
    "      x_{n}\n",
    "      \\end{bmatrix}\n",
    "      \\end{align}$$\n",
    "      \n",
    "We denote the $j$th column of $A$ by $a^j$ or $A_{:,j}$:\n",
    "\n",
    "$$A = \\begin{bmatrix}\n",
    "      | & | & | & ... & | \\\\\n",
    "      a^1 & a^2 & a^3 & ... & a^n \\\\\n",
    "      | & | & | & ... &|\n",
    "      \\end{bmatrix}$$\n",
    "      \n",
    "We denote the $i$th row of $A$ by $a^T$ or $A_{i,:}$:\n",
    "\n",
    "$$A = \\begin{bmatrix}\n",
    "      - a_{1}^T - \\\\\n",
    "      - a_{2}^T - \\\\\n",
    "      .. \\\\\n",
    "      - a_{m}^T - \n",
    "      \\end{bmatrix}$$\n",
    "**Matrix Multiplication**\n",
    "\n",
    "The product of two matrices $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{n \\times p}$ is the matrix\n",
    "\n",
    "$$\n",
    "C = AB \\in \\mathbb{R}^{m \\times p} \\enspace \\text{where, } C_{ij} = \\sum_{k=1}^{n}A_{ik}B_{kj}$$\n",
    "\n",
    "Note that in order for the matrix product to exist, the number of columns in $A$ must equal the number of rows in $B$.\n",
    "\n",
    "**Vector-Vector Multiplication**\n",
    "\n",
    "Given two vectors $x,y \\in \\mathbb{R}^n$, the quantity $x^Ty$, sometimes called the <span class = 'high'>inner product or dot product</span> of the vectors, is a real number given by\n",
    "\n",
    "$$\\begin{align} x^Ty \\in \\mathbb{R} = \\begin{matrix} [x_{1} & x_{2} & ... & x_{n}]\\end{matrix} \\begin{bmatrix}\n",
    "      y_{1} \\\\\n",
    "      y_{2} \\\\\n",
    "      .. \\\\\n",
    "      y_{n}\n",
    "      \\end{bmatrix} = \\sum_{i=1}^{n}x_{i}y_{i}\\end{align}$$\n",
    "      \n",
    "Given vectors $x \\in \\mathbb{R}^m, y \\in \\mathbb{R}^n$ (not necessarily of the same size), $xy^T \\in \\mathbb{R}^{m \\times n}$ is called the <span class = 'high'>outer product</span> of the vectors. It is a matrix, whose entries are given by $(xy^T)_{ij} = x_iy_j$:\n",
    "\n",
    "$$xy^T \\in \\mathbb{R}^{m \\times n} = \\begin{bmatrix}\n",
    "      x_{1} \\\\\n",
    "      x_{2} \\\\\n",
    "      .. \\\\\n",
    "      x_{n}\n",
    "      \\end{bmatrix}\\begin{matrix} [y_{1} & y_{2} & ... & y_{n}]\\end{matrix} = \\begin{bmatrix}\n",
    "      x_{1}y_{1} & x_{1}y_{2} & ... & x_{1}y_{n} \\\\\n",
    "      x_{2}y_{1} & x_{2}y_{2} & ... & x_{2}y_{n} \\\\\n",
    "      ... & ... & ... & ... \\\\\n",
    "      x_{m}y_{1} & x_{m}y_{2} & ... & x_{m}y_{n} \\\\\n",
    "      \\end{bmatrix}$$\n",
    "      \n",
    "**Matrix-Vector Products**\n",
    "\n",
    "Given a matrix $A \\in \\mathbb{R}^{m \\times n}$ and a vector $x \\in \\mathbb{R}^n$, their product is a vector $y = Ax \\in \\mathbb{R}^m$. There are a couple of ways of looking at matrix-vector multiplication, and we will look at each of them in turn.\n",
    "\n",
    "If we write $A$ by rows, then we can express $Ax$ as,\n",
    "\n",
    "$$y = Ax = \\begin{bmatrix}\n",
    "      - a_{1}^T - \\\\\n",
    "      - a_{2}^T - \\\\\n",
    "      .. \\\\\n",
    "      - a_{m}^T - \n",
    "      \\end{bmatrix}x = \\begin{bmatrix}\n",
    "      a_{1}^Tx\\\\\n",
    "      a_{2}^Tx \\\\\n",
    "      .. \\\\\n",
    "      a_{m}^Tx \n",
    "      \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb73188",
   "metadata": {},
   "source": [
    "In other words, the $i$th entry of $y$ is equal to the inner product of the $i$th row of $A$ and $x$, $y=a_i^Tx$.\n",
    "\n",
    "Alternatively, let's write $A$ in column form. In this case we see that,\n",
    "\n",
    "$$y = Ax = \\begin{bmatrix}\n",
    "      | & | & | & ... & | \\\\\n",
    "      a^1 & a^2 & a^3 & ... & a^n \\\\\n",
    "      | & | & | & ... &|\n",
    "      \\end{bmatrix} \\begin{bmatrix}\n",
    "      x_{1} \\\\\n",
    "      x_{2} \\\\\n",
    "      x_{3} \\\\\n",
    "      .. \\\\\n",
    "      x_{n}\n",
    "      \\end{bmatrix} = \\begin{matrix}\n",
    "      a^1\n",
    "      \\end{matrix}x_1+\\begin{matrix}\n",
    "      a^2\n",
    "      \\end{matrix}x_2+...+\\begin{matrix}\n",
    "      a^n\n",
    "      \\end{matrix}x_n$$\n",
    "      \n",
    "In other words, $y$ is a **linear combination** of the columns of $A$, where the coefficients of the linear combination are given by the entries of $x$.\n",
    "\n",
    "**Matrix-Matrix Products**\n",
    "\n",
    "Using the above information, we can view matrix-matrix multiplication from various perspectives. One of them is to view the matrix-matrix multiplication as a set of vector-vector products. The most obvious viewpoint, which follows immediately from the definition, is that the $(i,j)$th entry of $C$ is equal to the inner product of the $i$th row of $A$ and the $j$th column of $B$. Symbolically, this looks like the following:\n",
    "\n",
    "$$C = AB = \\begin{bmatrix}\n",
    "      - a_{1}^T - \\\\\n",
    "      - a_{2}^T - \\\\\n",
    "      .. \\\\\n",
    "      - a_{m}^T - \n",
    "      \\end{bmatrix}\\begin{bmatrix}\n",
    "      | & | & | & ... & | \\\\\n",
    "      b^1 & b^2 & b^3 & ... & b^n \\\\\n",
    "      | & | & | & ... &|\n",
    "      \\end{bmatrix}=\\begin{bmatrix}\n",
    "      a_1^Tb^1 & a_1^Tb^2 & ... & a_1^Tb^p \\\\\n",
    "      a_2^Tb^1 & a_2^Tb^2 & ... & a_2^Tb^p \\\\\n",
    "      .. & .. & ... & .. \\\\\n",
    "      a_m^Tb^1 & a_m^Tb^2 & ... & a_m^Tb^p\n",
    "      \\end{bmatrix}$$\n",
    "      \n",
    "Remember that since $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{n \\times p}, a_i \\in \\mathbb{R}^n \\text{ and } b^j \\in \\mathbb{R}^n$, so these inner products all make sense. This is the 'natural' representation when we represent $A$ by rows and $B$ by columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb11a484",
   "metadata": {},
   "source": [
    "**Symmetric Matrices**\n",
    "\n",
    "A square matrix $A \\in \\mathbb{R}^{n \\times n}$ is **symmetric** if $A=A^T$. It is **anti-symmetric** if $A=-A^T$. It is easy to show that any matrix $A \\in \\mathbb{R}^{n \\times n}$, the matrix $A+A^T$ is symmetric and the matrix $A-A^T$ is anti-symmetric. From this it follows that any square matrix $A \\in \\mathbb{R}^{n \\times n}$ can be be represented as a sum of a symmetric matrix and an anti-symmetric matrix as shown below:\n",
    "\n",
    "$$A = \\frac{1}{2}(A+A^T)+\\frac{1}{2}(A-A^T)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0d45ac",
   "metadata": {},
   "source": [
    "**Norms**\n",
    "\n",
    "A norm, $||x||$, of a vector is informally defined as the measure of 'length' of the vector. For example, we have the commonly used Eucledian or $l_{2}$ norm,\n",
    "\n",
    "$$||x||_{2} = \\sqrt{\\sum_{i=1}^{n}x_{i}^2}$$\n",
    "\n",
    "Note that $||x||_{2}^2 = x^Tx$\n",
    "More formally, norm is a function $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ that satisfies $4$ properties:\n",
    "\n",
    "<p style=\"line-height:180%;\">\n",
    "    \n",
    "$\\rightarrow \\text{For all } x \\in \\mathbb{R}^{n}, f(x) \\geq 0. \\text{ (non-negativity)}$\n",
    "    \n",
    "$\\rightarrow f(x)=0 \\text{ if and only if } x=0 \\text{ (definiteness)}$\n",
    "\n",
    "$\\rightarrow \\text{For all } x \\in \\mathbb{R}^{n}, t \\in \\mathbb{R}, f(tx) = |t|f(x). \\text{ (homogeneity)}$\n",
    "\n",
    "$\\rightarrow \\text{For all } x,y \\in \\mathbb{R}^{n}, f(x+y) \\leq f(x) +f(y). \\text{ (traingle inequality)}$\n",
    "</p>\n",
    "\n",
    "Other examples of norms are the $l_1$ norm,\n",
    "\n",
    "$$||x||_1 = \\sum_{i=1}^{n}|x_i|$$\n",
    "\n",
    "and the $l_\\infty$ norm,\n",
    "\n",
    "$$||x||_\\infty = max_i|x_i|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8bfb52",
   "metadata": {},
   "source": [
    "Norms can also be defined for matrices, such as the Frobenius norm,\n",
    "\n",
    "$$\\begin{align}||A||_{F} = \\sqrt{\\sum_{i=1}^{m}\\sum_{j=1}^{n}A_{ij}^2} = \\sqrt{trace(A^TA)}\\end{align}$$\n",
    "\n",
    "**Linear Independence and Rank**\n",
    "\n",
    "A set of vectors $\\{x_1, x_2,..,x_n\\} \\subset \\mathbb{R}^n$ is set to be **linearly independent** if no vector can be represented as a linear combination of the remaining vectors. Conversely, if one vector belonging to the set can be represented as a linear combination of the remaining vectors, then the vectors are said to be **linearly dependent**. That is, if\n",
    "\n",
    "$$x_n = \\sum_{i=1}^{n-1}\\alpha_i x_i$$\n",
    "\n",
    "for some scalar values $\\alpha_1,...,\\alpha_{n-1} \\in \\mathbb{R}$,then we say that the vectors $\\{x_1, x_2,..,x_n\\}$ are linearly dependent; otherwise, the vectors are linearly independent. For example, the vectors\n",
    "\n",
    "$$x_1=\\begin{bmatrix}\n",
    "1 \\\\\n",
    "2\\\\\n",
    "3\\end{bmatrix}\\enspace x_2 = \\begin{bmatrix}\n",
    "4 \\\\\n",
    "1\\\\ \n",
    "5 \\end{bmatrix}\\enspace x_3 = \\begin{bmatrix}\n",
    "2 \\\\\n",
    "-3 \\\\\n",
    "-1\\end{bmatrix}$$\n",
    "\n",
    "are linearly dependent because $x_3 = -2x_1+x_2$.\n",
    "\n",
    "_Column Rank_\n",
    "\n",
    "The column rank of a matrix $A \\in \\mathbb{R}^{m \\times n}$ is the size of the largest subset of columns of $A$ that constitute a linearly independent set.\n",
    "\n",
    "_Row Rank_\n",
    "\n",
    "The row rank of a matrix $A \\in \\mathbb{R}^{m \\times n}$ is the size of the largest subset of rows of $A$ that constitute a linearly independent set.\n",
    "\n",
    "If, for matrix column rank is equal to row rank, then both quantities are collectively referred to as the **rank of the matrix $A$**\n",
    "\n",
    "<p style=\"line-height:180%;\">\n",
    "    \n",
    "$\\rightarrow \\text{For } A \\in \\mathbb{R}^{m \\times n}, \\text{ rank}(A) \\leq min(m,n). \\text{ If rank}(A) = min(m,n), \\text{then } A \\text{ is said to be } \\textbf{full rank}$\n",
    "\n",
    "$\\rightarrow \\text{For } A \\in \\mathbb{R}^{m \\times n}, \\text{ rank}(A) = \\text{ rank}(A^T)$\n",
    "\n",
    "$\\rightarrow \\text{For } A,B \\in \\mathbb{R}^{m \\times n},\\text{ rank}(A+B) \\leq \\text{ rank}(A) + \\text{ rank}(B)$\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe5ac0f",
   "metadata": {},
   "source": [
    "**The Inverse of a Square Matrix**\n",
    "\n",
    "The **inverse** of a square matrix $A \\in \\mathbb{R}^{n \\times n}$ is denoted $A^{-1}$, and is the unique matrix such that:\n",
    "\n",
    "$$A^{-1}A=I=AA^{-1}$$\n",
    "\n",
    "Note that not all matrices have inverses. Non-square matrices, for example, do not have inverse by definition. However, for some square matrices $A$, it may stil be the case that $A^{-1}$ may not exist. In particular, we say that $A$ is **invertible** or **non-singular** if $A^{-1}$ exists and **non-invertible** or **singular** otherwise.\n",
    "\n",
    "In order for a square matrix $A$ to have an inverse $A^{-1}$, $A$ must be of full rank.\n",
    "\n",
    "The following are properties of the inverse; all assume that $A, B \\in \\mathbb{R}^{n \\times n}$ are non-singular:\n",
    "\n",
    "<p style=\"line-height:180%;\">\n",
    "    \n",
    "$\\rightarrow (A^{-1})^{-1}=A$\n",
    "\n",
    "$\\rightarrow (AB)^{-1} = B^{-1}A^{-1}$\n",
    "\n",
    "$\\rightarrow (A^{-1})^T = (A^T)^{-1}$\n",
    "\n",
    "$(A^T)^{-1}$ is denoted by $A^{-T}$\n",
    "</p>\n",
    "\n",
    "**Orthogonal Matrices**\n",
    "\n",
    "Two vectors $x,y \\in \\mathbb{R}^n$ are **orthogonal** if $x^Ty=0$. A vector $x \\in \\mathbb{R}^n$ is **normalized** if $||x||_2=1$. A square matrix $U \\in \\mathbb{R}^{n \\times n}$ is **orthogonal** _(note the different meanings when talking about vectors versus matrices)_ if all its columns are orthogonal to each other and are normalized.\n",
    "\n",
    "It follows immediately from the definition of orthogonality and normality that\n",
    "\n",
    "$$U^TU = I = UU^T$$\n",
    "\n",
    "In other words, the inverse of an orthogonal matrix is its transpose. Note that if $U$ is not square (i.e., $U \\in \\mathbb{R}^{m \\times n}, \\enspace n < m$) but its columns are still orthonormal, then $U^TU=I$, but $UU^T \\neq I$.\n",
    "\n",
    "Another nice property of orthogonal matrices is that operating on a vector with an orthogonal matrix will not change its _Euclidean norm_. i.e.,\n",
    "\n",
    "$$||Ux||_2 = ||x||_2$$\n",
    "\n",
    "for any $x \\in \\mathbb{R}^n, U \\in \\mathbb{R}^{n \\times n}$ orthogonal.\n",
    "\n",
    "**Range and nullspace of a Matrix**\n",
    "\n",
    "The span of a set of vectors $\\begin{Bmatrix} x_{1},x_{2},...x_{n} \\end{Bmatrix}$ is the set of all vectors that can be expressed as a linear combination of $\\begin{Bmatrix} x_{1},x_{2},...x_{n} \\end{Bmatrix}$. That is,\n",
    "\n",
    "$$\\text{span}(\\begin{Bmatrix} x_{1},x_{2},...x_{n} \\end{Bmatrix}) = \\left \\{ v:v = \\sum_{i=1}^{n} \\alpha_ix_i, \\enspace \\alpha_i \\in \\mathbb{R} \\right \\}$$\n",
    "\n",
    "It can be shown that if $\\begin{Bmatrix} x_{1},x_{2},...x_{n} \\end{Bmatrix}$ is a set of $n$ linearly independent vectors, where each $x_i \\in \\mathbb{R}^n$, then $\\text{span}(\\begin{Bmatrix} x_{1},x_{2},...x_{n} \\end{Bmatrix}) = \\mathbb{R}^n$. In other words, any vector $v \\in \\mathbb{R}^n$ can be written as a linear combination of $x_1$ through $x_n$.\n",
    "\n",
    "The **projection** of a vector $y \\in \\mathbb{R}^m$ onto the span of $\\begin{Bmatrix} x_{1},x_{2},...x_{n} \\end{Bmatrix}$ (here we assume $x_i \\in \\mathbb{R}^m$) is a vector $v \\in \\text{span}(\\begin{Bmatrix} x_{1},x_{2},...x_{n} \\end{Bmatrix})$, such that $v$ is as close as possible to $y$, as measured by the Euclidean norm $||v-y||_2$. We denote the projection as $\\text{Proj}(y;\\begin{Bmatrix} x_{1},x_{2},...x_{n} \\end{Bmatrix})$ and can define it formally as,\n",
    "\n",
    "$$\\text{Proj}(y;\\begin{Bmatrix}x_{1},...,x_{n}\\end{Bmatrix}) = \\text{argmin}_{v \\in span(\\begin{Bmatrix}x_{1},...,x_{n}\\end{Bmatrix})}||y-v||_{2}$$\n",
    "\n",
    "The range of a matrix $A \\in \\mathbb{R}^{m \\times n}$, denoted by $R(A)$, is the span of the columns of $A$. In other words,\n",
    "\n",
    "$$R(A) = \\begin{Bmatrix}v \\in \\mathbb{R}^m : v = Ax, x \\in \\mathbb{R}^n \\end{Bmatrix}$$\n",
    "\n",
    "The nullspace of a matrix $A \\in \\mathbb{R}^{m \\times n}$, denoted by $\\mathcal{N}(A)$ is the set of all vectors that equal to $0$ when multiplied by $A$, i.e.,\n",
    "\n",
    "$$\\mathcal{N}(A) = \\begin{Bmatrix} x \\in \\mathbb{R}^n : Ax = 0\\end{Bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a9e083",
   "metadata": {},
   "source": [
    "**Quadratic Forms and Positive Semidefinite Matrices**\n",
    "\n",
    "Given a square matrix $A \\in \\mathbb{R}^{n \\times n}$ and a vector $x \\in \\mathbb{R}^{n}$, the scalar value $x^TAx$ is called a _quadratic form_. Written explicitly, we see that\n",
    "\n",
    "$$\\begin{align}x^TAx = \\sum_{i=1}^{n}x_{i}(Ax)_{i} = \\sum_{i=1}^{n}x_{i}\\left (\\sum_{j=1}^{n}A_{ij}x_{j}\\right ) = \\sum_{i=1}^{n}\\sum_{j=1}^{n}A_{ij}x_{i}x_{j}\\end{align}$$\n",
    "\n",
    "Note that,\n",
    "\n",
    "$$x^TAx = (x^TAx)^T = x^TA^Tx = x^T \\left( \\frac{1}{2}A + \\frac{1}{2}A^T\\right)x$$\n",
    "\n",
    "where the first equality follows from the fact that the transpose of a scalar is equal to itself, and the second equality follows from the fact that we are averaging two quantities which are themselves equal. From this, we can conclude that only the symmetric part of $A$ contributes to the quadratic form. For this reason, we often implicitly assume that the matrices appearing in the quadratic form are symmetric.\n",
    "\n",
    "We have the following definitions:\n",
    "\n",
    "* A symmetric matrix $A \\in \\mathbb{S}^n$ is <span class = 'high'>positive definite</span> (PD) if for all non-zero vectors $x \\in \\mathbb{R}^n, x^TAx>0$. This is usually denoted $A\\succ0$, and often times the set of all positive definite matrices is denoted by $\\mathbb{S}_{++}^n$\n",
    "\n",
    "* A symmetric matrix $A \\in \\mathbb{S}^n$ is <span class = 'high'>positive semidefinite</span> (PSD) if for all non-zero vectors $x \\in \\mathbb{R}^n, x^TAx \\geq 0$. This is usually denoted $A \\succeq 0$, and often times the set of all positive semidefinite matrices is denoted by $\\mathbb{S}_{+}^n$\n",
    "\n",
    "* A symmetric matrix $A \\in \\mathbb{S}^n$ is <span class = 'high'>negative definite</span> (ND), denoted $A\\prec0$ if for all non-zero vectors $x \\in \\mathbb{R}^n, x^TAx<0$.\n",
    "\n",
    "* A symmetric matrix $A \\in \\mathbb{S}^n$ is <span class = 'high'>negative semidefinite</span> (NSD), denoted $A \\preceq 0$ if for all non-zero vectors $x \\in \\mathbb{R}^n, x^TAx \\leq 0$.\n",
    "\n",
    "* A symmetric matrix $A \\in \\mathbb{S}^n$ is <span class = 'high'>indefinite</span>, if it is neither positive semidefinite nor negative semidefinite - i.e., if there exists $x_{1}, x_{2} \\in \\mathbb{R}^n $ such that $x_{1}^TAx_{1}>0$ and $x_2^TAx_2<0$.\n",
    "\n",
    "It should be obvious that if $A$ is positive definite, then $-A$ is negative definite and vice versa. Likewise, if $A$ is positive semidefinite then $-A$ is negative semidefinite and vice versa. If $A$ is indefinite, then so is $-A$.\n",
    "\n",
    "One important property of positive definite and negative definite matrices is that they are always full rank, and hence, invertible. To see why this is the case, suppose that some matrix $A \\in \\mathbb{R}^{n \\times n}$ is not full rank. Then, suppose that the $j$th column of $A$ is expressible as a linear combination of other $n-1$ columns:\n",
    "\n",
    "$$a_j = \\sum_{i \\neq j} x_ia_i$$\n",
    "\n",
    "for some $x_1,..x_{j-1},x_{j+1},...,x_n \\in \\mathbb{R}$. Setting $x_j=-1$, we have\n",
    "\n",
    "$$Ax = \\sum_{i=1}^n x_ia_i=0$$\n",
    "\n",
    "But this implies $x^TAx=0$ for some non-zero vector $x$, so $A$ must be neither positive definite nor negative definite. Therefore, if $A$ is either positive definite or negative definite, it must be full rank.\n",
    "\n",
    "Finally, there is one type of positive definite matrix that comes up frequently, and so deserves some special mention. Given any matrix $A \\in \\mathbb{R}^{m \\times n}$ (not necessarily symmetric or even square), the matrix $G = A^TA$ (sometimes called a **Gram Matrix**) is always positive semidefinite. Further, if $m \\geq n$ (and we assume for convenience that A is full rank), then $G=A^TA$ is positive definite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace19fec",
   "metadata": {},
   "source": [
    "**Eigenvalues and Eigenvectors**\n",
    "\n",
    "Given a square matrix $A \\in \\mathbb{R}^{n \\times n}$, we say that $\\lambda \\in \\mathbb{C}$ is an **eigenvalue** if $A$ and $x \\in \\mathbb{C}^n$ is the corresponding **eigenvector** if "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1d095f",
   "metadata": {},
   "source": [
    "$$ Ax = \\lambda x, \\enspace x \\neq 0 $$\n",
    "\n",
    "Intuitively, this definition means that multiplying $A$ by the vector $x$ results in a new vector that points in the same direction as $x$, but scaled by a factor $\\lambda$. Also note that for any eigenvector $x \\in \\mathbb{C}^n$ and scalar $t \\in \\mathbb{C}, A(cx) = cAx = c \\lambda x = \\lambda (cx)$, so $cx$ is also an eigenvector. For this reason when we talk about 'the' eigenvector associated with $\\lambda$, we usually assume that the eigenvector is normalized to have length $1$.\n",
    "\n",
    "We can rewrite the equation above to state that $(\\lambda, x)$ is an eigenvalue-eigenvector pair of $A$ if,\n",
    "\n",
    "$$(\\lambda I -A)x =0, \\enspace x \\neq 0$$\n",
    "\n",
    "But $(\\lambda I -A)x =0$ has a non-zero solution to $x$ if and only if $(\\lambda I -A)$ has a non-empty nullspace, which is only the case if $(\\lambda I -A)$ is singular, i.e.,\n",
    "\n",
    "$$|(\\lambda I -A)|=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94886265",
   "metadata": {},
   "source": [
    "We can now use the previous definition of the determinant to expand this expression $|(\\lambda I -A)|$ into a polynomial in $\\lambda$, where $\\lambda$ will have degree $n$. It's often called the characteristic polynomial of the matrix $A$.\n",
    "\n",
    "We then find the $n$ roots of this characteristic polynomial and denote them by $\\lambda_1, \\lambda_2,...,\\lambda_n$. These are all the eigenvalues of the matrix $A$.\n",
    "\n",
    "The following are the properties of eigenvalues and eigenvectors:\n",
    "\n",
    "* The trace of A is equal to the sum of its eigenvalues,\n",
    "\n",
    "$$trace(A) = \\sum_{i=1}^n \\lambda_i$$\n",
    "\n",
    "* The determinant of $A$ is equal to the product of its eigenvalues,\n",
    "\n",
    "$$|A| = \\prod_{i=1}^n \\lambda_i$$\n",
    "\n",
    "* The rank of $A$ is equal to the number of non-zero eigenvalues of $A$.\n",
    "\n",
    "* Supppose $A$ is non-singular with eigenvalue $\\lambda$ and an associated eigenvector $x$. Then $\\frac{1}{\\lambda}$ is an eigenvalue of $A^{-1}$ with an associated eigenvector $x$, i.e., $A^{-1}x = (\\frac{1}{\\lambda})x$.\n",
    "\n",
    "* The eigenvalues of a diagonal matrix $D = \\text{diag}(d_1,...,d_n)$ are just the diagonal entries $d_1,...,d_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428e2929",
   "metadata": {},
   "source": [
    "**Eigenvalues and Eigenvectors of Symmetric Matrices**\n",
    "\n",
    "In general, the structures of the eigenvalues and eigenvectors of a general square matrix can be subtle to characterize. Fortunately, in most of the cases in machine learning, if suffices to deal with symmetric real matrices, whose eigenvalues and eigenvectors have remarkable properties.\n",
    "\n",
    "Throughout this section, let's assume that $A$ is a symmetric real matrix. We have the following properties:\n",
    "\n",
    "* All eigenvalues of $A$ are real numbers. We denote them by $\\lambda_1,...,\\lambda_n$\n",
    "\n",
    "* There exists a set of eigenvectors $u_1,...,u_n$ such that:\n",
    "a) for all $i, u_i$ is an eigenvector with eigenvalue $\\lambda_i$ and\n",
    "b) $u_1,...,u_n$ are unit vectors and orthogonal to each other.\n",
    "\n",
    "Let $U$ be the orthonormal matrix that contains $u_i$'s as columns:\n",
    "\n",
    "$$U = \\begin{bmatrix}\n",
    "      | & | & | & ... & | \\\\\n",
    "      u^1 & u^2 & u^3 & ... & u^n \\\\\n",
    "      | & | & | & ... &|\n",
    "      \\end{bmatrix}$$\n",
    "      \n",
    "Let $\\Lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a22b5cb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb79bad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36515d26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c61c236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee793fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}